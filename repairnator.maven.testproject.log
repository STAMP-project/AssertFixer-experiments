[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for net.mguenther.kafka:kafka-junit:jar:0.2.0
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ line 236, column 21
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Kafka JUnit Integration 0.2.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ kafka-junit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/mguenther/kafka-junit/395171737/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ kafka-junit ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ kafka-junit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ kafka-junit ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ kafka-junit ---
[INFO] Surefire report directory: /root/workspace/mguenther/kafka-junit/395171737/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.pom (3 KB at 5.2 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.12.4/surefire-providers-2.12.4.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.12.4/surefire-providers-2.12.4.pom (3 KB at 95.5 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.jar (37 KB at 1001.6 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running net.mguenther.kafka.junit.SendKeyValuesTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.156 sec
Running net.mguenther.kafka.junit.EmbeddedConnectConfigTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec
Running net.mguenther.kafka.junit.SendValuesTransactionalTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec
Running net.mguenther.kafka.junit.RecordProducerTest
2018-07-20 15:02:33 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:02:33 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:44400 which is assigned to the temporary directory /tmp/1532091753140-0.
2018-07-20 15:02:33 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:02:34 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit3751594742625100360/junit6384767600883527546/meta.properties
2018-07-20 15:02:35 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit3751594742625100360/junit6384767600883527546/meta.properties
2018-07-20 15:02:35 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:35 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:35 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit3751594742625100360/junit6384767600883527546.
2018-07-20 15:02:35 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:37020]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = true
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:02:35 INFO  KafkaProducer:336 - [Producer clientId=producer-1] Instantiated an idempotent producer.
2018-07-20 15:02:35 INFO  KafkaProducer:341 - [Producer clientId=producer-1] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
2018-07-20 15:02:35 INFO  KafkaProducer:341 - [Producer clientId=producer-1] Overriding the default acks to all since idempotence is enabled.
2018-07-20 15:02:35 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:35 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:35 WARN  NetworkClient:246 - [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:02:35 INFO  TransactionManager:346 - [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2018-07-20 15:02:35 INFO  KafkaProducer:341 - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:02:35 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:37020]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b449f836-f5a5-4135-98ee-b135d20c0165
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:02:35 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:35 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:41 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-1, groupId=b449f836-f5a5-4135-98ee-b135d20c0165] Discovered coordinator cyclone1:37020 (id: 2147482646 rack: null)
2018-07-20 15:02:41 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-1, groupId=b449f836-f5a5-4135-98ee-b135d20c0165] Revoking previously assigned partitions []
2018-07-20 15:02:41 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-1, groupId=b449f836-f5a5-4135-98ee-b135d20c0165] (Re-)joining group
2018-07-20 15:02:41 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-1, groupId=b449f836-f5a5-4135-98ee-b135d20c0165] Successfully joined group with generation 1
2018-07-20 15:02:41 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-1, groupId=b449f836-f5a5-4135-98ee-b135d20c0165] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:02:41 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:02:45 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@60cf80e7. This directory contains Kafka logs as well.
2018-07-20 15:02:45 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:02:45 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:44400 is stopping.
2018-07-20 15:02:45 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:44400 has been shut down.
2018-07-20 15:02:45 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:02:45 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:46197 which is assigned to the temporary directory /tmp/1532091765711-0.
2018-07-20 15:02:45 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:02:46 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit8784454330387243499/junit6112063740989680362/meta.properties
2018-07-20 15:02:46 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit8784454330387243499/junit6112063740989680362/meta.properties
2018-07-20 15:02:46 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:46 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:46 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit8784454330387243499/junit6112063740989680362.
2018-07-20 15:02:46 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:37539]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = true
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = e096209a-664f-4ef8-abe0-9cefa5d93799
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:02:46 INFO  KafkaProducer:336 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] Instantiated a transactional producer.
2018-07-20 15:02:46 INFO  KafkaProducer:341 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] Overriding the default acks to all since idempotence is enabled.
2018-07-20 15:02:46 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:46 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:46 INFO  TransactionManager:346 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] ProducerId set to -1 with epoch -1
2018-07-20 15:02:53 INFO  TransactionManager:346 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] ProducerId set to 0 with epoch 0
2018-07-20 15:02:53 WARN  NetworkClient:246 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] Error while fetching metadata with correlation id 38 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:02:53 WARN  NetworkClient:246 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] Error while fetching metadata with correlation id 39 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:02:53 INFO  KafkaProducer:341 - [Producer clientId=producer-2, transactionalId=e096209a-664f-4ef8-abe0-9cefa5d93799] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:02:53 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:37539]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b9882f24-aed1-4355-94e0-adaa0edf0fa2
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:02:53 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:02:53 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:02:59 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-2, groupId=b9882f24-aed1-4355-94e0-adaa0edf0fa2] Discovered coordinator cyclone1:37539 (id: 2147482646 rack: null)
2018-07-20 15:02:59 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-2, groupId=b9882f24-aed1-4355-94e0-adaa0edf0fa2] Revoking previously assigned partitions []
2018-07-20 15:02:59 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-2, groupId=b9882f24-aed1-4355-94e0-adaa0edf0fa2] (Re-)joining group
2018-07-20 15:02:59 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-2, groupId=b9882f24-aed1-4355-94e0-adaa0edf0fa2] Successfully joined group with generation 1
2018-07-20 15:02:59 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-2, groupId=b9882f24-aed1-4355-94e0-adaa0edf0fa2] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:02:59 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:03:05 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@544630b7. This directory contains Kafka logs as well.
2018-07-20 15:03:05 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:03:05 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:46197 is stopping.
2018-07-20 15:03:05 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:46197 has been shut down.
2018-07-20 15:03:05 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:03:05 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:41831 which is assigned to the temporary directory /tmp/1532091785150-0.
2018-07-20 15:03:05 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:03:05 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1594622274970838964/junit6852801053851609478/meta.properties
2018-07-20 15:03:06 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1594622274970838964/junit6852801053851609478/meta.properties
2018-07-20 15:03:06 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:06 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:06 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1594622274970838964/junit6852801053851609478.
2018-07-20 15:03:06 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44202]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:03:06 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:06 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:06 WARN  NetworkClient:246 - [Producer clientId=producer-3] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:06 WARN  NetworkClient:246 - [Producer clientId=producer-3] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:06 INFO  KafkaProducer:341 - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:03:06 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:44202]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8a0feed4-bea7-4b7f-becc-c97ce41b9a35
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:03:06 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:06 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:12 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-3, groupId=8a0feed4-bea7-4b7f-becc-c97ce41b9a35] Discovered coordinator cyclone1:44202 (id: 2147482646 rack: null)
2018-07-20 15:03:12 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-3, groupId=8a0feed4-bea7-4b7f-becc-c97ce41b9a35] Revoking previously assigned partitions []
2018-07-20 15:03:12 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-3, groupId=8a0feed4-bea7-4b7f-becc-c97ce41b9a35] (Re-)joining group
2018-07-20 15:03:12 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-3, groupId=8a0feed4-bea7-4b7f-becc-c97ce41b9a35] Successfully joined group with generation 1
2018-07-20 15:03:12 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-3, groupId=8a0feed4-bea7-4b7f-becc-c97ce41b9a35] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:03:12 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:03:15 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@486be205. This directory contains Kafka logs as well.
2018-07-20 15:03:15 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:03:15 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:41831 is stopping.
2018-07-20 15:03:15 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:41831 has been shut down.
2018-07-20 15:03:15 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:03:15 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:46813 which is assigned to the temporary directory /tmp/1532091795283-0.
2018-07-20 15:03:15 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:03:16 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit6330212664063203040/junit7291110567221869234/meta.properties
2018-07-20 15:03:16 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit6330212664063203040/junit7291110567221869234/meta.properties
2018-07-20 15:03:16 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:16 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:16 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit6330212664063203040/junit7291110567221869234.
2018-07-20 15:03:16 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:46419]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:03:16 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:16 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:16 WARN  NetworkClient:246 - [Producer clientId=producer-4] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:16 WARN  NetworkClient:246 - [Producer clientId=producer-4] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:16 INFO  KafkaProducer:341 - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:03:16 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:46419]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4151c4f4-b195-4f72-925c-b01e2d3d2328
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:03:16 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:16 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:23 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-4, groupId=4151c4f4-b195-4f72-925c-b01e2d3d2328] Discovered coordinator cyclone1:46419 (id: 2147482646 rack: null)
2018-07-20 15:03:23 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-4, groupId=4151c4f4-b195-4f72-925c-b01e2d3d2328] Revoking previously assigned partitions []
2018-07-20 15:03:23 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-4, groupId=4151c4f4-b195-4f72-925c-b01e2d3d2328] (Re-)joining group
2018-07-20 15:03:23 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-4, groupId=4151c4f4-b195-4f72-925c-b01e2d3d2328] Successfully joined group with generation 1
2018-07-20 15:03:23 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-4, groupId=4151c4f4-b195-4f72-925c-b01e2d3d2328] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:03:23 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:03:26 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@30c31dd7. This directory contains Kafka logs as well.
2018-07-20 15:03:26 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:03:26 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:46813 is stopping.
2018-07-20 15:03:26 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:46813 has been shut down.
2018-07-20 15:03:26 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:03:26 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:46085 which is assigned to the temporary directory /tmp/1532091806319-0.
2018-07-20 15:03:26 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:03:27 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit2180794180252872462/junit3816439964772132901/meta.properties
2018-07-20 15:03:27 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit2180794180252872462/junit3816439964772132901/meta.properties
2018-07-20 15:03:27 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:27 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:27 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit2180794180252872462/junit3816439964772132901.
2018-07-20 15:03:27 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:33057]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = true
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = 3c947ca3-e1c7-4d5b-a489-3bef79f886d0
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:03:27 INFO  KafkaProducer:336 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Instantiated a transactional producer.
2018-07-20 15:03:27 INFO  KafkaProducer:341 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Overriding the default acks to all since idempotence is enabled.
2018-07-20 15:03:27 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:27 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:27 INFO  TransactionManager:346 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] ProducerId set to -1 with epoch -1
2018-07-20 15:03:33 INFO  TransactionManager:346 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] ProducerId set to 0 with epoch 0
2018-07-20 15:03:33 WARN  NetworkClient:246 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Error while fetching metadata with correlation id 37 : {test-topic-1=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:33 WARN  NetworkClient:246 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Error while fetching metadata with correlation id 38 : {test-topic-1=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:34 WARN  NetworkClient:246 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Error while fetching metadata with correlation id 43 : {test-topic-2=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:34 WARN  NetworkClient:246 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Error while fetching metadata with correlation id 44 : {test-topic-2=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:34 INFO  KafkaProducer:341 - [Producer clientId=producer-5, transactionalId=3c947ca3-e1c7-4d5b-a489-3bef79f886d0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:03:34 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:33057]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d83febb3-cc19-453b-b6f4-79f02ecb6c47
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:03:34 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:34 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:40 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-5, groupId=d83febb3-cc19-453b-b6f4-79f02ecb6c47] Discovered coordinator cyclone1:33057 (id: 2147482646 rack: null)
2018-07-20 15:03:40 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-5, groupId=d83febb3-cc19-453b-b6f4-79f02ecb6c47] Revoking previously assigned partitions []
2018-07-20 15:03:40 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-5, groupId=d83febb3-cc19-453b-b6f4-79f02ecb6c47] (Re-)joining group
2018-07-20 15:03:40 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-5, groupId=d83febb3-cc19-453b-b6f4-79f02ecb6c47] Successfully joined group with generation 1
2018-07-20 15:03:40 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-5, groupId=d83febb3-cc19-453b-b6f4-79f02ecb6c47] Setting newly assigned partitions [test-topic-1-0]
2018-07-20 15:03:40 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:33057]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = a17b2ee5-bf32-481c-8cab-a0337c68375c
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:03:40 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:40 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:40 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-6, groupId=a17b2ee5-bf32-481c-8cab-a0337c68375c] Discovered coordinator cyclone1:33057 (id: 2147482646 rack: null)
2018-07-20 15:03:40 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-6, groupId=a17b2ee5-bf32-481c-8cab-a0337c68375c] Revoking previously assigned partitions []
2018-07-20 15:03:40 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-6, groupId=a17b2ee5-bf32-481c-8cab-a0337c68375c] (Re-)joining group
2018-07-20 15:03:40 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-6, groupId=a17b2ee5-bf32-481c-8cab-a0337c68375c] Successfully joined group with generation 1
2018-07-20 15:03:40 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-6, groupId=a17b2ee5-bf32-481c-8cab-a0337c68375c] Setting newly assigned partitions [test-topic-2-0]
2018-07-20 15:03:40 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:03:46 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@2d0566ba. This directory contains Kafka logs as well.
2018-07-20 15:03:46 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:03:46 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:46085 is stopping.
2018-07-20 15:03:46 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:46085 has been shut down.
2018-07-20 15:03:46 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:03:46 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:39613 which is assigned to the temporary directory /tmp/1532091826116-0.
2018-07-20 15:03:46 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:03:46 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit7800742032748680119/junit1568676278782871599/meta.properties
2018-07-20 15:03:47 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit7800742032748680119/junit1568676278782871599/meta.properties
2018-07-20 15:03:47 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:47 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:47 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit7800742032748680119/junit1568676278782871599.
2018-07-20 15:03:47 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:36810]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:03:47 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:47 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:47 WARN  NetworkClient:246 - [Producer clientId=producer-6] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:47 WARN  NetworkClient:246 - [Producer clientId=producer-6] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:03:47 INFO  KafkaProducer:341 - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:03:47 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:36810]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9ba0ed7e-910b-41b6-92da-b35da5a1802f
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:03:47 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:47 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:53 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-7, groupId=9ba0ed7e-910b-41b6-92da-b35da5a1802f] Discovered coordinator cyclone1:36810 (id: 2147482646 rack: null)
2018-07-20 15:03:53 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-7, groupId=9ba0ed7e-910b-41b6-92da-b35da5a1802f] Revoking previously assigned partitions []
2018-07-20 15:03:53 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-7, groupId=9ba0ed7e-910b-41b6-92da-b35da5a1802f] (Re-)joining group
2018-07-20 15:03:54 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-7, groupId=9ba0ed7e-910b-41b6-92da-b35da5a1802f] Successfully joined group with generation 1
2018-07-20 15:03:54 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-7, groupId=9ba0ed7e-910b-41b6-92da-b35da5a1802f] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:03:55 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:03:58 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@22ee2d0. This directory contains Kafka logs as well.
2018-07-20 15:03:58 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:03:58 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:39613 is stopping.
2018-07-20 15:03:58 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:39613 has been shut down.
2018-07-20 15:03:58 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:03:58 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:34354 which is assigned to the temporary directory /tmp/1532091838200-0.
2018-07-20 15:03:58 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:03:58 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit491515449318000804/junit6332102681603708493/meta.properties
2018-07-20 15:03:59 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit491515449318000804/junit6332102681603708493/meta.properties
2018-07-20 15:03:59 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:59 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:59 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit491515449318000804/junit6332102681603708493.
2018-07-20 15:03:59 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:33593]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = true
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = d9dcd46a-5198-45d3-a0f4-235cc7882249
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:03:59 INFO  KafkaProducer:336 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] Instantiated a transactional producer.
2018-07-20 15:03:59 INFO  KafkaProducer:341 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] Overriding the default acks to all since idempotence is enabled.
2018-07-20 15:03:59 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:03:59 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:03:59 INFO  TransactionManager:346 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] ProducerId set to -1 with epoch -1
2018-07-20 15:04:05 INFO  TransactionManager:346 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] ProducerId set to 0 with epoch 0
2018-07-20 15:04:05 WARN  NetworkClient:246 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] Error while fetching metadata with correlation id 36 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:05 WARN  NetworkClient:246 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] Error while fetching metadata with correlation id 37 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:05 INFO  KafkaProducer:341 - [Producer clientId=producer-7, transactionalId=d9dcd46a-5198-45d3-a0f4-235cc7882249] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:05 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:33593]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 61fc0193-c65f-4f2a-b6ee-da17391c234b
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:04:05 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:05 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:11 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-8, groupId=61fc0193-c65f-4f2a-b6ee-da17391c234b] Discovered coordinator cyclone1:33593 (id: 2147482646 rack: null)
2018-07-20 15:04:11 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-8, groupId=61fc0193-c65f-4f2a-b6ee-da17391c234b] Revoking previously assigned partitions []
2018-07-20 15:04:11 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-8, groupId=61fc0193-c65f-4f2a-b6ee-da17391c234b] (Re-)joining group
2018-07-20 15:04:11 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-8, groupId=61fc0193-c65f-4f2a-b6ee-da17391c234b] Successfully joined group with generation 1
2018-07-20 15:04:11 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-8, groupId=61fc0193-c65f-4f2a-b6ee-da17391c234b] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:04:11 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:04:16 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@2bffa76d. This directory contains Kafka logs as well.
2018-07-20 15:04:17 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:04:17 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:34354 is stopping.
2018-07-20 15:04:17 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:34354 has been shut down.
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 104.422 sec
Running net.mguenther.kafka.junit.EmbeddedZooKeeperConfigTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec
Running net.mguenther.kafka.junit.PropsTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running net.mguenther.kafka.junit.ReadKeyValuesTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running net.mguenther.kafka.junit.ObserveKeyValuesTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 sec
Running net.mguenther.kafka.junit.SendKeyValuesTransactionalTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec
Running net.mguenther.kafka.junit.RecordConsumerTest
2018-07-20 15:04:17 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:04:17 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:38190 which is assigned to the temporary directory /tmp/1532091857029-0.
2018-07-20 15:04:17 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:04:17 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit3727482416150801418/junit8766616540662875924/meta.properties
2018-07-20 15:04:18 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit3727482416150801418/junit8766616540662875924/meta.properties
2018-07-20 15:04:18 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:18 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:18 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit3727482416150801418/junit8766616540662875924.
2018-07-20 15:04:18 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44218]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:04:18 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:18 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:18 WARN  NetworkClient:246 - [Producer clientId=producer-8] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:18 WARN  NetworkClient:246 - [Producer clientId=producer-8] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:18 INFO  KafkaProducer:341 - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:18 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:44218]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d570f103-b945-423f-93bf-e55015524733
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:04:18 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:18 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:25 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-9, groupId=d570f103-b945-423f-93bf-e55015524733] Discovered coordinator cyclone1:44218 (id: 2147482646 rack: null)
2018-07-20 15:04:25 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-9, groupId=d570f103-b945-423f-93bf-e55015524733] Revoking previously assigned partitions []
2018-07-20 15:04:25 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-9, groupId=d570f103-b945-423f-93bf-e55015524733] (Re-)joining group
2018-07-20 15:04:25 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-9, groupId=d570f103-b945-423f-93bf-e55015524733] Successfully joined group with generation 1
2018-07-20 15:04:25 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-9, groupId=d570f103-b945-423f-93bf-e55015524733] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:04:27 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:04:31 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@5fa47fea. This directory contains Kafka logs as well.
2018-07-20 15:04:31 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:04:31 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:38190 is stopping.
2018-07-20 15:04:31 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:38190 has been shut down.
2018-07-20 15:04:31 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:04:31 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:39663 which is assigned to the temporary directory /tmp/1532091871164-0.
2018-07-20 15:04:31 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:04:31 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1282426988041154337/junit5376505368461640618/meta.properties
2018-07-20 15:04:32 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1282426988041154337/junit5376505368461640618/meta.properties
2018-07-20 15:04:32 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:32 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:32 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1282426988041154337/junit5376505368461640618.
2018-07-20 15:04:32 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44850]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:04:32 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:32 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:32 WARN  NetworkClient:246 - [Producer clientId=producer-9] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:32 WARN  NetworkClient:246 - [Producer clientId=producer-9] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:32 INFO  KafkaProducer:341 - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:32 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:44850]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0d314e14-e584-48fc-8099-2084e758f03a
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:04:32 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:32 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:38 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-10, groupId=0d314e14-e584-48fc-8099-2084e758f03a] Discovered coordinator cyclone1:44850 (id: 2147482646 rack: null)
2018-07-20 15:04:38 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-10, groupId=0d314e14-e584-48fc-8099-2084e758f03a] Revoking previously assigned partitions []
2018-07-20 15:04:38 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-10, groupId=0d314e14-e584-48fc-8099-2084e758f03a] (Re-)joining group
2018-07-20 15:04:38 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-10, groupId=0d314e14-e584-48fc-8099-2084e758f03a] Successfully joined group with generation 1
2018-07-20 15:04:38 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-10, groupId=0d314e14-e584-48fc-8099-2084e758f03a] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:04:40 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:04:42 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@5d10455d. This directory contains Kafka logs as well.
2018-07-20 15:04:43 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:04:43 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:39663 is stopping.
2018-07-20 15:04:43 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:39663 has been shut down.
2018-07-20 15:04:43 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:04:43 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:38447 which is assigned to the temporary directory /tmp/1532091883012-0.
2018-07-20 15:04:43 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:04:43 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit6742632405509116907/junit6551213182301571007/meta.properties
2018-07-20 15:04:44 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit6742632405509116907/junit6551213182301571007/meta.properties
2018-07-20 15:04:44 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:44 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:44 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit6742632405509116907/junit6551213182301571007.
2018-07-20 15:04:44 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:35470]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:04:44 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:44 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:44 WARN  NetworkClient:246 - [Producer clientId=producer-10] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:44 WARN  NetworkClient:246 - [Producer clientId=producer-10] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:44 INFO  KafkaProducer:341 - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:44 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:35470]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4344efcd-4636-469e-877f-6ad9878803d9
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:04:44 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:44 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:50 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-11, groupId=4344efcd-4636-469e-877f-6ad9878803d9] Discovered coordinator cyclone1:35470 (id: 2147482646 rack: null)
2018-07-20 15:04:50 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-11, groupId=4344efcd-4636-469e-877f-6ad9878803d9] Revoking previously assigned partitions []
2018-07-20 15:04:50 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-11, groupId=4344efcd-4636-469e-877f-6ad9878803d9] (Re-)joining group
2018-07-20 15:04:50 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-11, groupId=4344efcd-4636-469e-877f-6ad9878803d9] Successfully joined group with generation 1
2018-07-20 15:04:50 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-11, groupId=4344efcd-4636-469e-877f-6ad9878803d9] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:04:50 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:04:53 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@3af37506. This directory contains Kafka logs as well.
2018-07-20 15:04:53 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:04:53 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:38447 is stopping.
2018-07-20 15:04:53 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:38447 has been shut down.
2018-07-20 15:04:53 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:04:53 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:38836 which is assigned to the temporary directory /tmp/1532091893765-0.
2018-07-20 15:04:53 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:04:54 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1913928092972347304/junit1924468002419644704/meta.properties
2018-07-20 15:04:54 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1913928092972347304/junit1924468002419644704/meta.properties
2018-07-20 15:04:54 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:54 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:54 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1913928092972347304/junit1924468002419644704.
2018-07-20 15:04:54 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:34065]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:04:54 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:54 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:55 WARN  NetworkClient:246 - [Producer clientId=producer-11] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:55 WARN  NetworkClient:246 - [Producer clientId=producer-11] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:55 INFO  KafkaProducer:341 - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:55 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:34065]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.LongSerializer

2018-07-20 15:04:55 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:55 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:04:55 WARN  NetworkClient:246 - [Producer clientId=producer-12] Error while fetching metadata with correlation id 1 : {test-topic-key-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:55 WARN  NetworkClient:246 - [Producer clientId=producer-12] Error while fetching metadata with correlation id 3 : {test-topic-key-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:04:55 INFO  KafkaProducer:341 - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:04:55 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:34065]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 26450ff0-5e16-48fc-8eb2-2fa6230be508
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2018-07-20 15:04:55 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:04:55 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:01 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-12, groupId=26450ff0-5e16-48fc-8eb2-2fa6230be508] Discovered coordinator cyclone1:34065 (id: 2147482646 rack: null)
2018-07-20 15:05:01 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-12, groupId=26450ff0-5e16-48fc-8eb2-2fa6230be508] Revoking previously assigned partitions []
2018-07-20 15:05:01 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-12, groupId=26450ff0-5e16-48fc-8eb2-2fa6230be508] (Re-)joining group
2018-07-20 15:05:01 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-12, groupId=26450ff0-5e16-48fc-8eb2-2fa6230be508] Successfully joined group with generation 1
2018-07-20 15:05:01 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-12, groupId=26450ff0-5e16-48fc-8eb2-2fa6230be508] Setting newly assigned partitions [test-topic-key-value-types-0]
2018-07-20 15:05:01 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:05:06 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@6813a331. This directory contains Kafka logs as well.
2018-07-20 15:05:06 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:05:06 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:38836 is stopping.
2018-07-20 15:05:06 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:38836 has been shut down.
2018-07-20 15:05:06 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:05:06 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:41037 which is assigned to the temporary directory /tmp/1532091906172-0.
2018-07-20 15:05:06 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:05:06 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit7476966751050811015/junit5025273632266037822/meta.properties
2018-07-20 15:05:07 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit7476966751050811015/junit5025273632266037822/meta.properties
2018-07-20 15:05:07 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:07 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:07 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit7476966751050811015/junit5025273632266037822.
2018-07-20 15:05:07 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44065]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:05:07 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:07 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:07 WARN  NetworkClient:246 - [Producer clientId=producer-13] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:07 WARN  NetworkClient:246 - [Producer clientId=producer-13] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:07 INFO  KafkaProducer:341 - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:07 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44065]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer

2018-07-20 15:05:07 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:07 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:07 WARN  NetworkClient:246 - [Producer clientId=producer-14] Error while fetching metadata with correlation id 1 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:08 WARN  NetworkClient:246 - [Producer clientId=producer-14] Error while fetching metadata with correlation id 3 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:08 INFO  KafkaProducer:341 - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:08 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:44065]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 18af408a-9f59-4c90-9f59-eb1ca021d52b
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer

2018-07-20 15:05:08 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:08 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:14 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-13, groupId=18af408a-9f59-4c90-9f59-eb1ca021d52b] Discovered coordinator cyclone1:44065 (id: 2147482646 rack: null)
2018-07-20 15:05:14 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-13, groupId=18af408a-9f59-4c90-9f59-eb1ca021d52b] Revoking previously assigned partitions []
2018-07-20 15:05:14 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-13, groupId=18af408a-9f59-4c90-9f59-eb1ca021d52b] (Re-)joining group
2018-07-20 15:05:14 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-13, groupId=18af408a-9f59-4c90-9f59-eb1ca021d52b] Successfully joined group with generation 1
2018-07-20 15:05:14 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-13, groupId=18af408a-9f59-4c90-9f59-eb1ca021d52b] Setting newly assigned partitions [test-topic-key-value-filter-0]
2018-07-20 15:05:14 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:05:18 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@2375b321. This directory contains Kafka logs as well.
2018-07-20 15:05:18 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:05:18 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:41037 is stopping.
2018-07-20 15:05:18 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:41037 has been shut down.
2018-07-20 15:05:18 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:05:18 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:46124 which is assigned to the temporary directory /tmp/1532091918762-0.
2018-07-20 15:05:18 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:05:19 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1238523407391026388/junit845034687524464338/meta.properties
2018-07-20 15:05:19 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1238523407391026388/junit845034687524464338/meta.properties
2018-07-20 15:05:19 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:19 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:19 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1238523407391026388/junit845034687524464338.
2018-07-20 15:05:19 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41883]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:05:19 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:19 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:20 WARN  NetworkClient:246 - [Producer clientId=producer-15] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:20 WARN  NetworkClient:246 - [Producer clientId=producer-15] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:20 INFO  KafkaProducer:341 - [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:20 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41883]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.LongSerializer

2018-07-20 15:05:20 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:20 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:20 WARN  NetworkClient:246 - [Producer clientId=producer-16] Error while fetching metadata with correlation id 1 : {test-topic-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:20 WARN  NetworkClient:246 - [Producer clientId=producer-16] Error while fetching metadata with correlation id 3 : {test-topic-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:20 INFO  KafkaProducer:341 - [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:20 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:41883]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d7261ef8-971f-4a40-ad7a-ece514598832
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2018-07-20 15:05:20 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:20 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:27 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-14, groupId=d7261ef8-971f-4a40-ad7a-ece514598832] Discovered coordinator cyclone1:41883 (id: 2147482646 rack: null)
2018-07-20 15:05:27 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-14, groupId=d7261ef8-971f-4a40-ad7a-ece514598832] Revoking previously assigned partitions []
2018-07-20 15:05:27 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-14, groupId=d7261ef8-971f-4a40-ad7a-ece514598832] (Re-)joining group
2018-07-20 15:05:27 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-14, groupId=d7261ef8-971f-4a40-ad7a-ece514598832] Successfully joined group with generation 1
2018-07-20 15:05:27 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-14, groupId=d7261ef8-971f-4a40-ad7a-ece514598832] Setting newly assigned partitions [test-topic-value-types-0]
2018-07-20 15:05:27 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:05:32 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@648ee871. This directory contains Kafka logs as well.
2018-07-20 15:05:32 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:05:32 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:46124 is stopping.
2018-07-20 15:05:32 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:46124 has been shut down.
2018-07-20 15:05:32 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:05:32 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:44555 which is assigned to the temporary directory /tmp/1532091932086-0.
2018-07-20 15:05:32 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:05:32 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5138843936497278240/junit7874706757865828888/meta.properties
2018-07-20 15:05:33 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5138843936497278240/junit7874706757865828888/meta.properties
2018-07-20 15:05:33 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:33 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:33 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit5138843936497278240/junit7874706757865828888.
2018-07-20 15:05:33 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:40662]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:05:33 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:33 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:33 WARN  NetworkClient:246 - [Producer clientId=producer-17] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:33 WARN  NetworkClient:246 - [Producer clientId=producer-17] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:33 INFO  KafkaProducer:341 - [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:33 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:40662]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ba45097d-4c8b-431e-8256-51e17c5bc2df
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2018-07-20 15:05:33 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:33 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:39 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-15, groupId=ba45097d-4c8b-431e-8256-51e17c5bc2df] Discovered coordinator cyclone1:40662 (id: 2147482646 rack: null)
2018-07-20 15:05:39 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-15, groupId=ba45097d-4c8b-431e-8256-51e17c5bc2df] Revoking previously assigned partitions []
2018-07-20 15:05:39 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-15, groupId=ba45097d-4c8b-431e-8256-51e17c5bc2df] (Re-)joining group
2018-07-20 15:05:39 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-15, groupId=ba45097d-4c8b-431e-8256-51e17c5bc2df] Successfully joined group with generation 1
2018-07-20 15:05:39 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-15, groupId=ba45097d-4c8b-431e-8256-51e17c5bc2df] Setting newly assigned partitions [test-topic-0]
2018-07-20 15:05:41 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:05:45 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@2e52fb3e. This directory contains Kafka logs as well.
2018-07-20 15:05:45 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:05:45 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:44555 is stopping.
2018-07-20 15:05:45 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:44555 has been shut down.
2018-07-20 15:05:45 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:05:45 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:39545 which is assigned to the temporary directory /tmp/1532091945327-0.
2018-07-20 15:05:45 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:05:46 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5313120354273084903/junit8603928917133269222/meta.properties
2018-07-20 15:05:46 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5313120354273084903/junit8603928917133269222/meta.properties
2018-07-20 15:05:46 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:46 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:46 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit5313120354273084903/junit8603928917133269222.
2018-07-20 15:05:46 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:33136]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:05:46 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:46 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:46 WARN  NetworkClient:246 - [Producer clientId=producer-18] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:46 WARN  NetworkClient:246 - [Producer clientId=producer-18] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:46 INFO  KafkaProducer:341 - [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:46 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:33136]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.LongSerializer

2018-07-20 15:05:46 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:46 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:47 WARN  NetworkClient:246 - [Producer clientId=producer-19] Error while fetching metadata with correlation id 1 : {test-topic-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:47 WARN  NetworkClient:246 - [Producer clientId=producer-19] Error while fetching metadata with correlation id 3 : {test-topic-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:05:47 INFO  KafkaProducer:341 - [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:05:47 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:33136]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b8712263-e601-4044-8389-d3a277f5ee6c
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2018-07-20 15:05:47 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:05:47 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:05:53 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-16, groupId=b8712263-e601-4044-8389-d3a277f5ee6c] Discovered coordinator cyclone1:33136 (id: 2147482646 rack: null)
2018-07-20 15:05:53 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-16, groupId=b8712263-e601-4044-8389-d3a277f5ee6c] Revoking previously assigned partitions []
2018-07-20 15:05:53 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-16, groupId=b8712263-e601-4044-8389-d3a277f5ee6c] (Re-)joining group
2018-07-20 15:05:53 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-16, groupId=b8712263-e601-4044-8389-d3a277f5ee6c] Successfully joined group with generation 1
2018-07-20 15:05:53 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-16, groupId=b8712263-e601-4044-8389-d3a277f5ee6c] Setting newly assigned partitions [test-topic-value-types-0]
2018-07-20 15:05:55 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:05:59 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@2abc224d. This directory contains Kafka logs as well.
2018-07-20 15:05:59 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:05:59 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:39545 is stopping.
2018-07-20 15:05:59 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:39545 has been shut down.
2018-07-20 15:05:59 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:05:59 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:43058 which is assigned to the temporary directory /tmp/1532091959081-0.
2018-07-20 15:05:59 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:05:59 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1857265872172283534/junit5527528586342118735/meta.properties
2018-07-20 15:06:00 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1857265872172283534/junit5527528586342118735/meta.properties
2018-07-20 15:06:00 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:00 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:00 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1857265872172283534/junit5527528586342118735.
2018-07-20 15:06:00 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44845]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:06:00 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:00 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:00 WARN  NetworkClient:246 - [Producer clientId=producer-20] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:00 WARN  NetworkClient:246 - [Producer clientId=producer-20] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:00 INFO  KafkaProducer:341 - [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:00 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:44845]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.LongSerializer

2018-07-20 15:06:00 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:00 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:00 WARN  NetworkClient:246 - [Producer clientId=producer-21] Error while fetching metadata with correlation id 1 : {test-topic-key-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:01 WARN  NetworkClient:246 - [Producer clientId=producer-21] Error while fetching metadata with correlation id 3 : {test-topic-key-value-types=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:01 INFO  KafkaProducer:341 - [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:01 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:44845]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = a2d58339-57e2-45de-907d-640fbe8a5ad1
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2018-07-20 15:06:01 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:01 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:07 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-17, groupId=a2d58339-57e2-45de-907d-640fbe8a5ad1] Discovered coordinator cyclone1:44845 (id: 2147482646 rack: null)
2018-07-20 15:06:07 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-17, groupId=a2d58339-57e2-45de-907d-640fbe8a5ad1] Revoking previously assigned partitions []
2018-07-20 15:06:07 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-17, groupId=a2d58339-57e2-45de-907d-640fbe8a5ad1] (Re-)joining group
2018-07-20 15:06:07 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-17, groupId=a2d58339-57e2-45de-907d-640fbe8a5ad1] Successfully joined group with generation 1
2018-07-20 15:06:07 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-17, groupId=a2d58339-57e2-45de-907d-640fbe8a5ad1] Setting newly assigned partitions [test-topic-key-value-types-0]
2018-07-20 15:06:08 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:06:13 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@79a1728c. This directory contains Kafka logs as well.
2018-07-20 15:06:13 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:06:13 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:43058 is stopping.
2018-07-20 15:06:13 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:43058 has been shut down.
2018-07-20 15:06:13 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:06:13 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:38015 which is assigned to the temporary directory /tmp/1532091973080-0.
2018-07-20 15:06:13 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:06:13 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5160715525592165231/junit1705106383377032184/meta.properties
2018-07-20 15:06:14 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5160715525592165231/junit1705106383377032184/meta.properties
2018-07-20 15:06:14 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:14 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:14 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit5160715525592165231/junit1705106383377032184.
2018-07-20 15:06:14 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41781]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:06:14 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:14 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:14 WARN  NetworkClient:246 - [Producer clientId=producer-22] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:14 WARN  NetworkClient:246 - [Producer clientId=producer-22] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:14 INFO  KafkaProducer:341 - [Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:14 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41781]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer

2018-07-20 15:06:14 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:14 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:14 WARN  NetworkClient:246 - [Producer clientId=producer-23] Error while fetching metadata with correlation id 1 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:15 WARN  NetworkClient:246 - [Producer clientId=producer-23] Error while fetching metadata with correlation id 3 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:15 INFO  KafkaProducer:341 - [Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:15 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:41781]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4bdbafbd-94fd-4985-8c89-a171cd2ff569
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer

2018-07-20 15:06:15 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:15 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:21 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-18, groupId=4bdbafbd-94fd-4985-8c89-a171cd2ff569] Discovered coordinator cyclone1:41781 (id: 2147482646 rack: null)
2018-07-20 15:06:21 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-18, groupId=4bdbafbd-94fd-4985-8c89-a171cd2ff569] Revoking previously assigned partitions []
2018-07-20 15:06:21 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-18, groupId=4bdbafbd-94fd-4985-8c89-a171cd2ff569] (Re-)joining group
2018-07-20 15:06:21 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-18, groupId=4bdbafbd-94fd-4985-8c89-a171cd2ff569] Successfully joined group with generation 1
2018-07-20 15:06:21 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-18, groupId=4bdbafbd-94fd-4985-8c89-a171cd2ff569] Setting newly assigned partitions [test-topic-key-value-filter-0]
2018-07-20 15:06:22 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:06:25 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@24097e9b. This directory contains Kafka logs as well.
2018-07-20 15:06:25 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:06:25 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:38015 is stopping.
2018-07-20 15:06:25 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:38015 has been shut down.
2018-07-20 15:06:25 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:06:25 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:33756 which is assigned to the temporary directory /tmp/1532091985496-0.
2018-07-20 15:06:25 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:06:26 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1591289700715851922/junit4580632315770248348/meta.properties
2018-07-20 15:06:26 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1591289700715851922/junit4580632315770248348/meta.properties
2018-07-20 15:06:26 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:26 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:26 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1591289700715851922/junit4580632315770248348.
2018-07-20 15:06:26 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:37756]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:06:26 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:26 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:26 WARN  NetworkClient:246 - [Producer clientId=producer-24] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:26 WARN  NetworkClient:246 - [Producer clientId=producer-24] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:27 INFO  KafkaProducer:341 - [Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:27 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:37756]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer

2018-07-20 15:06:27 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:27 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:27 WARN  NetworkClient:246 - [Producer clientId=producer-25] Error while fetching metadata with correlation id 1 : {test-topic-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:27 WARN  NetworkClient:246 - [Producer clientId=producer-25] Error while fetching metadata with correlation id 3 : {test-topic-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:27 INFO  KafkaProducer:341 - [Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:27 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:37756]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 11bf7c91-7569-4fab-ae12-3466d429b0ce
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer

2018-07-20 15:06:27 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:27 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:33 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-19, groupId=11bf7c91-7569-4fab-ae12-3466d429b0ce] Discovered coordinator cyclone1:37756 (id: 2147482646 rack: null)
2018-07-20 15:06:33 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-19, groupId=11bf7c91-7569-4fab-ae12-3466d429b0ce] Revoking previously assigned partitions []
2018-07-20 15:06:33 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-19, groupId=11bf7c91-7569-4fab-ae12-3466d429b0ce] (Re-)joining group
2018-07-20 15:06:33 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-19, groupId=11bf7c91-7569-4fab-ae12-3466d429b0ce] Successfully joined group with generation 1
2018-07-20 15:06:33 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-19, groupId=11bf7c91-7569-4fab-ae12-3466d429b0ce] Setting newly assigned partitions [test-topic-value-filter-0]
2018-07-20 15:06:35 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:06:38 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@25a73de1. This directory contains Kafka logs as well.
2018-07-20 15:06:38 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:06:38 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:33756 is stopping.
2018-07-20 15:06:38 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:33756 has been shut down.
2018-07-20 15:06:38 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:06:38 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:37908 which is assigned to the temporary directory /tmp/1532091998745-0.
2018-07-20 15:06:38 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:06:39 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit810635701125081832/junit9169365535258035936/meta.properties
2018-07-20 15:06:39 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit810635701125081832/junit9169365535258035936/meta.properties
2018-07-20 15:06:39 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:39 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:39 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit810635701125081832/junit9169365535258035936.
2018-07-20 15:06:39 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:43186]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:06:39 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:39 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:40 WARN  NetworkClient:246 - [Producer clientId=producer-26] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:40 WARN  NetworkClient:246 - [Producer clientId=producer-26] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:40 INFO  KafkaProducer:341 - [Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:40 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:43186]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer

2018-07-20 15:06:40 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:40 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:40 WARN  NetworkClient:246 - [Producer clientId=producer-27] Error while fetching metadata with correlation id 1 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:40 WARN  NetworkClient:246 - [Producer clientId=producer-27] Error while fetching metadata with correlation id 3 : {test-topic-key-value-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:40 INFO  KafkaProducer:341 - [Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:40 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:43186]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = e9837f9b-00e6-4a14-933b-addd983ed51c
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer

2018-07-20 15:06:40 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:40 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:46 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-20, groupId=e9837f9b-00e6-4a14-933b-addd983ed51c] Discovered coordinator cyclone1:43186 (id: 2147482646 rack: null)
2018-07-20 15:06:46 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-20, groupId=e9837f9b-00e6-4a14-933b-addd983ed51c] Revoking previously assigned partitions []
2018-07-20 15:06:46 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-20, groupId=e9837f9b-00e6-4a14-933b-addd983ed51c] (Re-)joining group
2018-07-20 15:06:46 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-20, groupId=e9837f9b-00e6-4a14-933b-addd983ed51c] Successfully joined group with generation 1
2018-07-20 15:06:46 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-20, groupId=e9837f9b-00e6-4a14-933b-addd983ed51c] Setting newly assigned partitions [test-topic-key-value-filter-0]
2018-07-20 15:06:48 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:06:51 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@773e2eb5. This directory contains Kafka logs as well.
2018-07-20 15:06:51 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:06:51 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:37908 is stopping.
2018-07-20 15:06:51 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:37908 has been shut down.
2018-07-20 15:06:51 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:06:51 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:36211 which is assigned to the temporary directory /tmp/1532092011560-0.
2018-07-20 15:06:51 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:06:52 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1504409974336921598/junit1767881535503027472/meta.properties
2018-07-20 15:06:52 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit1504409974336921598/junit1767881535503027472/meta.properties
2018-07-20 15:06:52 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:52 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:52 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit1504409974336921598/junit1767881535503027472.
2018-07-20 15:06:52 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41805]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2018-07-20 15:06:52 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:52 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:53 WARN  NetworkClient:246 - [Producer clientId=producer-28] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:53 WARN  NetworkClient:246 - [Producer clientId=producer-28] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:53 INFO  KafkaProducer:341 - [Producer clientId=producer-28] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:53 INFO  ProducerConfig:223 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [:41805]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer

2018-07-20 15:06:53 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:53 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:53 WARN  NetworkClient:246 - [Producer clientId=producer-29] Error while fetching metadata with correlation id 1 : {test-topic-key-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:53 WARN  NetworkClient:246 - [Producer clientId=producer-29] Error while fetching metadata with correlation id 3 : {test-topic-key-filter=LEADER_NOT_AVAILABLE}
2018-07-20 15:06:53 INFO  KafkaProducer:341 - [Producer clientId=producer-29] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2018-07-20 15:06:53 INFO  ConsumerConfig:223 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [:41805]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 026161c6-7e12-4bb2-937d-6d9f4f9c22fa
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer

2018-07-20 15:06:53 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:06:53 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:06:59 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-21, groupId=026161c6-7e12-4bb2-937d-6d9f4f9c22fa] Discovered coordinator cyclone1:41805 (id: 2147482646 rack: null)
2018-07-20 15:06:59 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-21, groupId=026161c6-7e12-4bb2-937d-6d9f4f9c22fa] Revoking previously assigned partitions []
2018-07-20 15:06:59 INFO  AbstractCoordinator:336 - [Consumer clientId=consumer-21, groupId=026161c6-7e12-4bb2-937d-6d9f4f9c22fa] (Re-)joining group
2018-07-20 15:06:59 INFO  AbstractCoordinator:341 - [Consumer clientId=consumer-21, groupId=026161c6-7e12-4bb2-937d-6d9f4f9c22fa] Successfully joined group with generation 1
2018-07-20 15:06:59 INFO  ConsumerCoordinator:341 - [Consumer clientId=consumer-21, groupId=026161c6-7e12-4bb2-937d-6d9f4f9c22fa] Setting newly assigned partitions [test-topic-key-filter-0]
2018-07-20 15:07:01 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:07:05 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@1e6dad8. This directory contains Kafka logs as well.
2018-07-20 15:07:05 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:07:05 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:36211 is stopping.
2018-07-20 15:07:05 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:36211 has been shut down.
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 168.858 sec
Running net.mguenther.kafka.junit.EmbeddedKafkaConfigTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec
Running net.mguenther.kafka.junit.SendValuesTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec
Running net.mguenther.kafka.junit.TopicManagerTest
2018-07-20 15:07:05 INFO  EmbeddedZooKeeper:24 - Embedded ZooKeeper is starting.
2018-07-20 15:07:05 INFO  EmbeddedZooKeeper:26 - Successfully started an embedded ZooKeeper instance at 127.0.0.1:46272 which is assigned to the temporary directory /tmp/1532092025891-0.
2018-07-20 15:07:05 INFO  EmbeddedKafka:48 - Embedded Kafka broker is starting.
2018-07-20 15:07:06 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5662060510053634753/junit3457919638897663931/meta.properties
2018-07-20 15:07:06 WARN  BrokerMetadataCheckpoint:87 - No meta.properties file under dir /tmp/junit5662060510053634753/junit3457919638897663931/meta.properties
2018-07-20 15:07:06 INFO  AppInfoParser:109 - Kafka version : 1.0.0
2018-07-20 15:07:06 INFO  AppInfoParser:110 - Kafka commitId : aaa7af6d4a11b29d
2018-07-20 15:07:06 INFO  EmbeddedKafka:53 - The embedded Kafka broker has been started. Its logs can be found at /tmp/junit5662060510053634753/junit3457919638897663931.
2018-07-20 15:07:07 INFO  DefaultTopicManager:35 - Created topic 'test-topic' with settings TopicConfig(topic=test-topic, numberOfPartitions=1, numberOfReplicas=1, properties={delete.retention.ms=86400000, min.insync.replicas=1, cleanup.policy=delete}).
2018-07-20 15:07:07 INFO  DefaultTopicManager:55 - Marked topic 'test-topic' for deletion.
2018-07-20 15:07:07 INFO  EmbeddedKafka:67 - The embedded Kafka broker is stopping.
2018-07-20 15:07:07 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001 epoch 1 fails to send request (type=StopReplicaRequest, controllerId=1001, controllerEpoch=1, deletePartitions=true, partitions=test-topic-0) to broker cyclone1:45037 (id: 1001 rack: null). Reconnecting to broker.
java.io.IOException: Connection to 1001 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:95)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:230)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:07 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:07 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:07 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:07 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:07 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:07 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:08 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:08 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:09 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:09 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:09 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:09 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:09 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:09 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:09 WARN  NetworkClient:241 - [Controller id=1001, targetBrokerId=1001] Connection to node 1001 could not be established. Broker may not be available.
2018-07-20 15:07:09 WARN  RequestSendThread:93 - [Controller-1001-to-broker-1001-send-thread]: Controller 1001's connection to broker cyclone1:45037 (id: 1001 rack: null) was unsuccessful
java.io.IOException: Connection to cyclone1:45037 (id: 1001 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:269)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:223)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)
2018-07-20 15:07:09 INFO  EmbeddedKafka:70 - Removing working directory at org.junit.rules.TemporaryFolder@108bdbd8. This directory contains Kafka logs as well.
2018-07-20 15:07:09 INFO  EmbeddedKafka:72 - The embedded Kafka broker has been stopped.
2018-07-20 15:07:09 INFO  EmbeddedZooKeeper:43 - The embedded ZooKeeper instance at 127.0.0.1:46272 is stopping.
2018-07-20 15:07:09 INFO  EmbeddedZooKeeper:45 - The embedded ZooKeeper instance at 127.0.0.1:46272 has been shut down.
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.515 sec
Running net.mguenther.kafka.junit.KeyValueTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec
Running net.mguenther.kafka.junit.ExternalKafkaClusterTest
2018-07-20 15:07:09 INFO  DockerClientProviderStrategy:182 - Will use 'netty' transport
2018-07-20 15:07:19 ERROR EnvironmentAndSystemPropertyClientProviderStrategy:44 - ping failed with configuration Environment variables, system properties and defaults. Resolved: 
    dockerHost=unix:///var/run/docker.sock
    apiVersion='{UNKNOWN_VERSION}'
    registryUrl='https://index.docker.io/v1/'
    registryUsername='root'
    registryPassword='null'
    registryEmail='null'
    dockerConfig='DefaultDockerClientConfig[dockerHost=unix:///var/run/docker.sock,registryUsername=root,registryPassword=<null>,registryEmail=<null>,registryUrl=https://index.docker.io/v1/,dockerConfigPath=/root/.docker,sslConfig=<null>,apiVersion={UNKNOWN_VERSION},dockerConfig=<null>]'
 due to org.rnorth.ducttape.TimeoutException: Timeout waiting for result with exception
org.rnorth.ducttape.TimeoutException: Timeout waiting for result with exception
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:51)
	at org.testcontainers.dockerclient.DockerClientProviderStrategy.ping(DockerClientProviderStrategy.java:189)
	at org.testcontainers.dockerclient.EnvironmentAndSystemPropertyClientProviderStrategy.test(EnvironmentAndSystemPropertyClientProviderStrategy.java:42)
	at org.testcontainers.dockerclient.DockerClientProviderStrategy.lambda$getFirstValidStrategy$2(DockerClientProviderStrategy.java:112)
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267)
	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:302)
	at java.util.stream.Streams$ConcatSpliterator.tryAdvance(Streams.java:731)
	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:469)
	at org.testcontainers.dockerclient.DockerClientProviderStrategy.getFirstValidStrategy(DockerClientProviderStrategy.java:147)
	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:100)
	at org.testcontainers.containers.GenericContainer.<init>(GenericContainer.java:149)
	at org.testcontainers.containers.KafkaContainer.<init>(KafkaContainer.java:27)
	at org.testcontainers.containers.KafkaContainer.<init>(KafkaContainer.java:23)
	at net.mguenther.kafka.junit.ExternalKafkaClusterTest.<init>(ExternalKafkaClusterTest.java:14)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: org.testcontainers.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No such file or directory: /var/run/docker.sock
	at org.testcontainers.shaded.io.netty.channel.unix.Errors.throwConnectException(Errors.java:107)
	at org.testcontainers.shaded.io.netty.channel.unix.Socket.connect(Socket.java:243)
	at org.testcontainers.shaded.io.netty.channel.epoll.AbstractEpollStreamChannel.doConnect(AbstractEpollStreamChannel.java:734)
	at org.testcontainers.shaded.io.netty.channel.epoll.EpollDomainSocketChannel.doConnect(EpollDomainSocketChannel.java:87)
	at org.testcontainers.shaded.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.connect(AbstractEpollStreamChannel.java:797)
	at org.testcontainers.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1274)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)
	at org.testcontainers.shaded.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.connect(CombinedChannelDuplexHandler.java:497)
	at org.testcontainers.shaded.io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at org.testcontainers.shaded.io.netty.channel.CombinedChannelDuplexHandler.connect(CombinedChannelDuplexHandler.java:298)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:512)
	at org.testcontainers.shaded.io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:993)
	at org.testcontainers.shaded.io.netty.channel.AbstractChannel.connect(AbstractChannel.java:255)
	at org.testcontainers.shaded.io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:252)
	at org.testcontainers.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at org.testcontainers.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at org.testcontainers.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:312)
	at org.testcontainers.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at org.testcontainers.shaded.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: connect(..) failed: No such file or directory
	... 23 more
2018-07-20 15:07:19 ERROR DockerClientProviderStrategy:149 - Could not find a valid Docker environment. Please check configuration. Attempted configurations were:
2018-07-20 15:07:19 ERROR DockerClientProviderStrategy:151 -     EnvironmentAndSystemPropertyClientProviderStrategy: failed with exception InvalidConfigurationException (ping failed)
2018-07-20 15:07:19 ERROR DockerClientProviderStrategy:151 -     UnixSocketClientProviderStrategy: failed with exception InvalidConfigurationException (ping failed). Root cause NoSuchFileException (/var/run/docker.sock)
2018-07-20 15:07:19 ERROR DockerClientProviderStrategy:153 - As no valid configuration was found, execution cannot continue
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.432 sec <<< FAILURE!
externalKafkaClusterShouldWorkWithExternalResources(net.mguenther.kafka.junit.ExternalKafkaClusterTest)  Time elapsed: 0.005 sec  <<< ERROR!
java.lang.IllegalStateException: Could not find a valid Docker environment. Please see logs and check configuration
	at org.testcontainers.dockerclient.DockerClientProviderStrategy.lambda$getFirstValidStrategy$3(DockerClientProviderStrategy.java:156)
	at java.util.Optional.orElseThrow(Optional.java:290)
	at org.testcontainers.dockerclient.DockerClientProviderStrategy.getFirstValidStrategy(DockerClientProviderStrategy.java:148)
	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:100)
	at org.testcontainers.containers.GenericContainer.<init>(GenericContainer.java:149)
	at org.testcontainers.containers.KafkaContainer.<init>(KafkaContainer.java:27)
	at org.testcontainers.containers.KafkaContainer.<init>(KafkaContainer.java:23)
	at net.mguenther.kafka.junit.ExternalKafkaClusterTest.<init>(ExternalKafkaClusterTest.java:14)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)


Results :

Tests in error: 
  externalKafkaClusterShouldWorkWithExternalResources(net.mguenther.kafka.junit.ExternalKafkaClusterTest): Could not find a valid Docker environment. Please see logs and check configuration

Tests run: 66, Failures: 0, Errors: 1, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:50 min
[INFO] Finished at: 2018-07-20T15:07:20+02:00
[INFO] Final Memory: 16M/295M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default-test) on project kafka-junit: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/mguenther/kafka-junit/395171737/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
